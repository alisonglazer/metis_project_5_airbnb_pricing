{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alison Glazer**\n",
    "# Airbnb Pricing - Natural Language Processing\n",
    "We will look at the topics discussed in the names and descriptions of each Airbnb listing to see if that affects the prices set by the hosts. We will first focus primarily on the discussion of the style and design of each listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Display-Options\" data-toc-modified-id=\"Display-Options-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Display Options</a></span></li><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Analyze-the-Text\" data-toc-modified-id=\"Analyze-the-Text-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Analyze the Text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Helper-Functions-for-Preprocessing\" data-toc-modified-id=\"Helper-Functions-for-Preprocessing-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Helper Functions for Preprocessing</a></span></li><li><span><a href=\"#Preprocess\" data-toc-modified-id=\"Preprocess-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Preprocess</a></span></li></ul></li><li><span><a href=\"#Topic-Modeling\" data-toc-modified-id=\"Topic-Modeling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Topic Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#LSA\" data-toc-modified-id=\"LSA-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>LSA</a></span></li><li><span><a href=\"#NMF\" data-toc-modified-id=\"NMF-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>NMF</a></span></li><li><span><a href=\"#LDA\" data-toc-modified-id=\"LDA-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>LDA</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Text\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Saving\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors sourced from here: https://usbrandcolors.com/airbnb-colors/\n",
    "bnb_red = '#FF5A5F'\n",
    "bnb_blue = '#00A699'\n",
    "bnb_orange = '#FC642D'\n",
    "bnb_lgrey = '#767676'\n",
    "bnb_dgrey = '#484848'\n",
    "bnb_maroon = '#92174D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Seattle Listings Features\n",
    "with open('data/lax_text_feat.pickle', 'rb') as to_read:\n",
    "    text_feat = pickle.load(to_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess a corpus (Series) of documents before using a vectorizer\n",
    "    - remove numbers and punctuation\n",
    "    - remove urls\n",
    "    - convert all text to lower case\n",
    "    \"\"\"\n",
    "    # Remove numbers and punctuation\n",
    "    alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "    \n",
    "    # remove non alpha characters\n",
    "    non_alpha = lambda x: re.sub('[^a-zA-Z]', ' ', x)\n",
    "    \n",
    "#     # Remove all image links\n",
    "#     image_link = lambda x: re.sub('http.*?Â¦',' ', x)\n",
    "    \n",
    "    return docs.map(alphanumeric).map(punc_lower).map(non_alpha)#.map(image_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    '''\n",
    "    Stem word tokens using the English SnowballStemmer\n",
    "    '''\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    stemmed = [w for w in stemmed if len(w) > 3]\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "# Stop words\n",
    "stop_words_pre = [\n",
    "    'room', 'bedroom', 'home', 'location', 'live', 'guest', 'house', 'queen',\n",
    "    'neighborhood', 'floor', 'place', 'minute', 'block', 'stay', 'area',\n",
    "    'city', 'away', 'unit', 'include', 'available', 'just', 'need', 'apart',\n",
    "    'local', 'shop', 'downtown', 'space', 'like', 'build', 'rail', 'washer',\n",
    "    'dryer', 'microwave', 'stock', 'universe', 'walk', 'enjoy', 'union',\n",
    "    'olympic', 'shared', 'size', 'street', 'access', 'close', 'free', 'bath',\n",
    "    'note', 'sugar', 'creamer', 'iron', 'earplug', 'fridge', 'kitchenette',\n",
    "    'laptop', 'appliance', 'airport', 'suite', 'park', 'mile', 'recycle',\n",
    "    'compost', 'lincoln', 'leed', 'platinum', 'ferry', 'ferries', 'does',\n",
    "    'wifi', 'check', 'condo', 'make', 'hill', 'bathroom', 'kitchen', 'grocery',\n",
    "    'studio', 'coffee', 'puget', 'condominium', 'salt', 'pepper', 'bathroom',\n",
    "    'shower', 'lake', 'heart', 'center', 'offer', 'dine', 'dining', 'provide',\n",
    "    'high', 'fully', 'door', 'welcome', 'features', 'feature', 'feel', 'main',\n",
    "    'line', 'window', 'level', 'laundry', 'universal', 'right', 'university',\n",
    "    'link', 'private', 'small', 'large', 'restaurants', 'located', 'living',\n",
    "    'apartment', 'great', 'guests', 'minutes', 'blocks', 'perfect', 'building',\n",
    "    'bedrooms', 'stocked', 'amenities', 'basement', 'near', 'district',\n",
    "    'airbnb', 'short', 'plenty', 'sofa', 'includes', 'equipped', 'closet',\n",
    "    'anne', 'separate', 'water', 'sized', 'best', 'mattress', 'love',\n",
    "    'station', 'time', 'table', 'west', 'amazon', 'people', 'entire', 'couch',\n",
    "    'towels', 'yard', 'upstairs', 'provided', 'miles', 'ride', 'parks',\n",
    "    'travelers', 'lower', 'steps', 'loft', 'green', 'central', 'market',\n",
    "    'rooftop', 'including', 'windows', 'floors', 'drive', 'lots', 'beds',\n",
    "    'nearby', 'brand', 'easy', 'entrance', 'phone', 'public', 'questions',\n",
    "    'night', 'appliances', 'want', 'text', 'south', 'stores', 'offers',\n",
    "    'square', 'help', 'speed', 'walls', 'brick', 'site', 'apartments', 'super',\n",
    "    'explore', 'composting', 'futon', 'essentials', 'double', 'elevator',\n",
    "    'master', 'king', 'furnished', 'cottage', 'craftsman', 'venues', 'radius',\n",
    "    'town', 'sound', 'pioneer', 'internet', 'expect', 'townhouse', 'sink',\n",
    "    'shampoo', 'linens', 'conditioner', 'mini', 'original', 'coffee_maker',\n",
    "    'property', 'broadcast', 'host', 'site', 'cable', 'come', 'north', 'entry',\n",
    "    'extra', 'lines', 'long', 'neighborhoods', 'second', 'convention', 'body',\n",
    "    'wash', 'artist', 'press', 'french', 'wake', 'columbia', 'base', 'filled',\n",
    "    'community', 'waterfront', 'looking', 'spot', 'know', 'little', 'rental',\n",
    "    'furniture', 'term', 'foam', 'memory', 'oven', 'plus', 'baker', 'stadiums',\n",
    "    'quick', 'excited', 'mountains', 'good', 'cooking', 'surrounded', 'north',\n",
    "    'electric', 'rainer', 'junction', 'trail', 'taxi', 'speed', 'walls',\n",
    "    'brick', 'site', 'apartments', 'super', 'explore', 'composting', 'futon',\n",
    "    'essentials', 'double', 'elevator', 'master', 'king', 'furnished',\n",
    "    'cottage', 'craftsman', 'venues', 'radius', 'town', 'sound', 'pioneer',\n",
    "    'internet', 'expect', 'townhouse', 'sink', 'shampoo', 'linens',\n",
    "    'conditioner', 'mini', 'belltown', 'douglas', 'original', 'coffee_maker',\n",
    "    'property', 'broadcast', 'cottage', 'host', 'site', 'cable', 'come',\n",
    "    'north', 'entry', 'extra', 'lines', 'long', 'neighborhoods', 'second',\n",
    "    'convention', 'body', 'wash', 'artist', 'press', 'french', 'wake',\n",
    "    'columbia', 'base', 'filled', 'community', 'waterfront', 'looking', 'spot',\n",
    "    'know', 'little', 'rental', 'furniture', 'term', 'foam', 'memory', 'oven',\n",
    "    'plus', 'baker', 'stadiums', 'quick', 'excited', 'mountains', 'good',\n",
    "    'cooking', 'surrounded', 'north', 'electric', 'rainer', 'junction',\n",
    "    'trail', 'taxi', 'refrigerator', 'youtube', 'zipcar', 'zoka', 'zeus',\n",
    "    'desk', 'hair', 'train', 'loop', 'international', 'train', 'issue',\n",
    "    'friendly', 'efficient', 'support', 'hosts', 'enter', 'bathrooms',\n",
    "    'townhome', 'ground', 'conditioning', 'feeling', 'mins', 'ceilings',\n",
    "    'built', 'othello', 'experience', 'fresh', 'elliott', 'flat', 'guide',\n",
    "    'things', 'managed', 'locally', 'selection', 'madison', 'cafes', 'plan',\n",
    "    'doors', 'concept', 'sleeps', 'wheel', 'sunset', 'common', 'nice', 'food',\n",
    "    'having', 'begin', 'centrally', 'flat', 'screen', 'read', 'madrona',\n",
    "    'lock', 'common', 'guide', 'commute', 'hospitals', 'wide', 'safeco',\n",
    "    'foot', 'museum', 'stairs', 'hospital', 'burke', 'gilman', 'stop',\n",
    "    'absolutely', 'week', 'steel', 'wood', 'stainless', 'oxford', 'self',\n",
    "    'dedicated', 'beans', 'roasted', 'rooms', 'dishwasher', 'stove',\n",
    "    'mountain', 'skyline', 'breakfast', 'hardwood', 'major', 'areas',\n",
    "    'breweries', 'buses', 'mall', 'routes', 'tree', 'bedding', 'smart', 'bike',\n",
    "    'mind', 'cotton', 'golden', 'watch', 'lotion', 'grinder', 'amenity',\n",
    "    'summer', 'glass', 'provides', 'story', 'needed', 'covered', 'chairs',\n",
    "    'forward', 'reading', 'pillows', 'sheets', 'storage', 'counter', 'organic',\n",
    "    'board', 'utensils', 'arrival', 'ridge', 'score', 'walking', 'message',\n",
    "    'goes', 'days', 'book', 'listing', 'booking', 'quite', 'needs',\n",
    "    'greenwood', 'fitness', 'indoor', 'field', 'century', 'hollywood',\n",
    "    'kinney', 'santa', 'monica', 'los', 'angeles', 'downtown', 'hills',\n",
    "    'avenue', 'ocean', 'beverly', 'manhattan', 'hermosa', 'redondo',\n",
    "    'wilshire', 'universal', 'airport', 'culver', 'venice', 'marina', 'canals',\n",
    "    'abbott', 'boardwalk', 'blvd', 'metro', 'dtla', 'canyon', 'malibu',\n",
    "    'studios', 'bungalow', 'pier', 'pacific', 'belmont', 'california', 'wine',\n",
    "    'rose', 'maker', 'number', 'twin', 'hidden', 'staples', 'female',\n",
    "    'koreatown', 'burbank', 'valley', 'warner', 'brothers', 'noho', 'arts',\n",
    "    'feliz', 'glendale', 'echo', 'silver', 'silverlake', 'highland',\n",
    "    'griffith', 'observatory', 'uber', 'grove', 'melrose', 'included',\n",
    "    'allowed', 'forum', 'ucla', 'westwood', 'village', 'brentwood', 'college',\n",
    "    'stadium', 'hall', 'court', 'tennis', 'pasadena', 'freeway', 'rose',\n",
    "    'topanga', 'sign', 'walt', 'mary', 'abbot', 'chinese', 'distance',\n",
    "    'promenade', 'world', 'spots', 'trader', 'joes', 'berry', 'bikes',\n",
    "    'ralphs', 'washington', 'playa', 'muscle', 'spanish', 'attractions',\n",
    "    'grill', 'https', 'means', 'americana', 'mattresses', 'concert', 'parking',\n",
    "    'hotel', 'reviews', 'stays', 'month', 'listings', 'years', 'minimum',\n",
    "    'website', 'kettle', 'possible', 'flags', 'magic', 'person', 'important',\n",
    "    'additional', 'accommodate', 'accommodates', 'starbucks', 'store', 'bars',\n",
    "    'shops', 'disneyland', 'soap', 'dishes', 'huge', 'famous', 'shopping',\n",
    "    'shops', 'disney', 'total', 'using', 'inch', 'target', 'reservation',\n",
    "    'premises', 'freeways', 'toaster', 'pots', 'pans', 'complimentary',\n",
    "    'garage', 'aquarium', 'sand', 'shore', 'surf', 'path', 'boards'\n",
    "    'fame'\n",
    "]\n",
    "stop_words = stop_words_pre\n",
    "# stop_words = stem_tokens(\n",
    "#     list(preprocess(pd.Series(stop_words_pre))),\n",
    "#     nltk.SnowballStemmer(\"english\", ignore_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pos_filter = ['JJ','JJR','NN','NNS','RB','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "def tokenize_and_pos(text):\n",
    "    '''\n",
    "    Tokenize and only keep words in the specified list of parts of speech\n",
    "    '''\n",
    "    tokens = [x[0] for x in pos_tag(word_tokenize(text)) if x[1] in list_pos_filter]\n",
    "\n",
    "#     mwe_tokenizer = MWETokenizer(\n",
    "#         mwes=[('walking',\n",
    "#                'distance'), ('fully', 'furnished'), (\n",
    "#                    'private',\n",
    "#                    'entrance'), ('coffee', 'maker'), ('pike', 'place',\n",
    "#                                                       'market')])\n",
    "#     tokens = mwe_tokenizer.tokenize(tokens)\n",
    "\n",
    "    #Alphabetical tokens only with word length greater than 3\n",
    "    tokens = [w for w in tokens if len(w) > 3]\n",
    "#     stems = stem_tokens(tokens, stemmer)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Stop words\n",
    "# my_stop_words = stem_tokens(text.ENGLISH_STOP_WORDS.union(set(stop_words)),\n",
    "#                             stemmer)\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(set(stop_words))\n",
    "\n",
    "def countvec(docs,\n",
    "             tokenizer=tokenize_and_pos,\n",
    "             ngram_range=(2, 3),\n",
    "             stop_words=my_stop_words,\n",
    "             min_df=10,\n",
    "             max_df=0.9):\n",
    "    \"\"\"\n",
    "    Generate document-term inputs for topic modeling with LSA and NMF(doc_term_mat) and LDA(corpus, id2word) using count-vectorizer\n",
    "    ----\n",
    "    Input: series of documents (strings)\n",
    "    Output: Document-term matrix\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(tokenizer=tokenizer,\n",
    "                                       ngram_range=ngram_range,\n",
    "                                       stop_words=stop_words,\n",
    "                                       min_df=min_df,\n",
    "                                       max_df=max_df)\n",
    "    count_vectorizer.fit(docs)\n",
    "\n",
    "    # Create document-term matrix for use in LSA and NMF\n",
    "    doc_term_mat = count_vectorizer.transform(docs)\n",
    "\n",
    "    return count_vectorizer, doc_term_mat\n",
    "\n",
    "\n",
    "def tfidfvec(docs,\n",
    "             tokenizer=tokenize_and_pos,\n",
    "             ngram_range=(2, 3),\n",
    "             stop_words=my_stop_words,\n",
    "             min_df=10,\n",
    "             max_df=0.9):\n",
    "    \"\"\"\n",
    "    Generate document-term inputs for topic modeling with LSA and NMF(doc_term_mat) and LDA(corpus, id2word) using tf-idf-vectorizer\n",
    "    ----\n",
    "    Input: series of documents (strings)\n",
    "    Output: Document-term matrix\n",
    "    \"\"\"\n",
    "    tf_vectorizer = TfidfVectorizer(tokenizer=tokenizer,\n",
    "                                    ngram_range=ngram_range,\n",
    "                                    stop_words=stop_words,\n",
    "                                    min_df=min_df,\n",
    "                                    max_df=max_df)\n",
    "    tf_vectorizer.fit(docs)\n",
    "\n",
    "    # Create document-term matrix for use in LSA and NMF\n",
    "    doc_term_mat = tf_vectorizer.transform(docs)\n",
    "\n",
    "    return tf_vectorizer, doc_term_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"\"\"\n",
    "    Display topics and top associated words given a topic model\n",
    "    \"\"\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "vectorizer_count, doc_term_mat_count = countvec(preprocess(text_feat),ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14522, 4402)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat_count.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.DataFrame(doc_term_mat_count.toarray(), columns=vectorizer_count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbey</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abode</th>\n",
       "      <th>abound</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abundance</th>\n",
       "      <th>abundant</th>\n",
       "      <th>academy</th>\n",
       "      <th>accent</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 4402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbey  ability  able  abode  abound  absolute  abundance  abundant  \\\n",
       "0      0        0     0      0       0         0          0         0   \n",
       "1      0        0     0      0       0         0          0         0   \n",
       "2      0        0     0      0       0         0          0         0   \n",
       "3      0        0     0      0       0         0          0         0   \n",
       "4      0        0     0      0       0         0          0         0   \n",
       "\n",
       "   academy  accent  ...  yellow  yoga  yogurt  york  young  younger  yummy  \\\n",
       "0        0       0  ...       0     0       0     0      0        0      0   \n",
       "1        0       0  ...       0     0       0     0      0        0      0   \n",
       "2        0       0  ...       0     0       0     0      0        0      0   \n",
       "3        0       0  ...       0     0       0     0      0        0      0   \n",
       "4        0       0  ...       0     0       0     0      0        0      0   \n",
       "\n",
       "   zero  zone  zuma  \n",
       "0     0     0     0  \n",
       "1     0     0     0  \n",
       "2     0     0     0  \n",
       "3     0     0     0  \n",
       "4     0     0     0  \n",
       "\n",
       "[5 rows x 4402 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beach             16573\n",
       "quiet              8331\n",
       "beautiful          7751\n",
       "modern             6614\n",
       "pool               6360\n",
       "comfortable        6358\n",
       "cozy               6353\n",
       "spacious           6224\n",
       "clean              5805\n",
       "patio              5360\n",
       "views              4287\n",
       "netflix            4002\n",
       "safe               3773\n",
       "open               3675\n",
       "view               3629\n",
       "outdoor            3614\n",
       "light              3413\n",
       "newly              3373\n",
       "family             3248\n",
       "business           3167\n",
       "amazing            3011\n",
       "bright             2961\n",
       "backyard           2954\n",
       "luxury             2933\n",
       "style              2863\n",
       "garden             2826\n",
       "balcony            2779\n",
       "renovated          2663\n",
       "remodeled          2589\n",
       "relax              2482\n",
       "work               2441\n",
       "gated              2250\n",
       "privacy            2227\n",
       "comfy              2162\n",
       "charming           2158\n",
       "fame               1992\n",
       "couples            1957\n",
       "deck               1896\n",
       "convenient         1874\n",
       "sleep              1826\n",
       "prime              1817\n",
       "natural            1767\n",
       "peaceful           1747\n",
       "relaxing           1719\n",
       "historic           1686\n",
       "entertainment      1660\n",
       "lovely             1651\n",
       "gorgeous           1649\n",
       "transportation     1640\n",
       "beaches            1640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See which words are used the most\n",
    "vocab.sum(axis=0).sort_values(ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beach             4150\n",
       "quiet             3725\n",
       "beautiful         3315\n",
       "cozy              2965\n",
       "comfortable       2878\n",
       "modern            2691\n",
       "spacious          2680\n",
       "clean             2509\n",
       "patio             2230\n",
       "pool              2121\n",
       "netflix           1904\n",
       "safe              1862\n",
       "open              1740\n",
       "light             1620\n",
       "outdoor           1617\n",
       "view              1595\n",
       "views             1578\n",
       "business          1566\n",
       "newly             1532\n",
       "family            1510\n",
       "amazing           1461\n",
       "bright            1396\n",
       "backyard          1346\n",
       "style             1311\n",
       "luxury            1284\n",
       "work              1278\n",
       "relax             1250\n",
       "privacy           1237\n",
       "renovated         1187\n",
       "balcony           1180\n",
       "remodeled         1122\n",
       "charming          1077\n",
       "comfy             1067\n",
       "garden            1060\n",
       "gated             1041\n",
       "convenient         993\n",
       "couples            977\n",
       "transportation     945\n",
       "sleep              891\n",
       "smoking            889\n",
       "peaceful           874\n",
       "fame               869\n",
       "relaxing           860\n",
       "natural            856\n",
       "prime              856\n",
       "entertainment      854\n",
       "beaches            824\n",
       "lovely             812\n",
       "deck               778\n",
       "single             775\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.astype(bool).sum(axis=0).sort_values(ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF-IDF Vectorizer\n",
    "# vectorizer_tf, doc_term_mat_tf= tfidfvec(preprocess(text_feat),ngram_range=(1,2))\n",
    "# doc_term_mat_tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_tf = pd.DataFrame(doc_term_mat_tf.toarray(), columns=vectorizer_tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio per topic:\n",
      " [0.03762693 0.02846406 0.01549927 0.01325538 0.01151103 0.01000239\n",
      " 0.00933672 0.00901816 0.00851796 0.00812501 0.00787523 0.00769106\n",
      " 0.00716445 0.00685841 0.00644821 0.00598066 0.00551982 0.00542835\n",
      " 0.00527348 0.00505858]\n",
      "\n",
      "Topic  0\n",
      "beach, quiet, beautiful, modern, comfortable, cozy, spacious, patio, pool, clean\n",
      "\n",
      "Topic  1\n",
      "beach, shoreline, boards, boogie, pike, breezes, peninsula, silicon, alamitos, strand\n",
      "\n",
      "Topic  2\n",
      "pool, views, luxury, jacuzzi, view, balcony, beach, swimming, outdoor, modern\n",
      "\n",
      "Topic  3\n",
      "modern, spacious, netflix, luxury, views, newly, open, hulu, renovated, comfortable\n",
      "\n",
      "Topic  4\n",
      "beautiful, patio, garden, views, outdoor, deck, open, view, light, trees\n",
      "\n",
      "Topic  5\n",
      "beautiful, clean, comfortable, spacious, balcony, business, amazing, view, fame, cozy\n",
      "\n",
      "Topic  6\n",
      "quiet, modern, views, beautiful, safe, luxury, balcony, amazing, peaceful, beach\n",
      "\n",
      "Topic  7\n",
      "views, spacious, view, quiet, balcony, amazing, deck, comfortable, panoramic, breathtaking\n",
      "\n",
      "Topic  8\n",
      "views, cozy, clean, comfortable, view, amazing, deck, garden, panoramic, star\n",
      "\n",
      "Topic  9\n",
      "netflix, hulu, prime, business, luxury, cozy, quiet, couples, hdtv, solo\n",
      "\n",
      "Topic  10\n",
      "clean, patio, netflix, hulu, safe, views, beautiful, prime, hdtv, luxury\n",
      "\n",
      "Topic  11\n",
      "comfortable, quiet, amazing, price, business, garden, outdoor, historic, netflix, fame\n",
      "\n",
      "Topic  12\n",
      "business, couples, solo, adventurers, newly, families, family, renovated, remodeled, kids\n",
      "\n",
      "Topic  13\n",
      "newly, renovated, remodeled, backyard, outdoor, open, light, guesthouse, deck, family\n",
      "\n",
      "Topic  14\n",
      "garden, light, open, outdoor, bright, netflix, clean, natural, backyard, hulu\n",
      "\n",
      "Topic  15\n",
      "view, luxury, garden, balcony, amazing, style, safe, open, gated, fame\n",
      "\n",
      "Topic  16\n",
      "family, outdoor, backyard, safe, luxury, style, friends, luxurious, single, beaches\n",
      "\n",
      "Topic  17\n",
      "garden, luxury, safe, style, renovated, spacious, gorgeous, designer, resort, cozy\n",
      "\n",
      "Topic  18\n",
      "outdoor, style, luxury, light, open, amazing, natural, seating, quiet, bright\n",
      "\n",
      "Topic  19\n",
      "outdoor, view, garden, newly, balcony, spacious, modern, netflix, bright, cozy\n"
     ]
    }
   ],
   "source": [
    "lsa_c = TruncatedSVD(20)\n",
    "doc_topic_lsa_c = lsa_c.fit_transform(doc_term_mat_count)\n",
    "print('Explained Variance Ratio per topic:\\n',lsa_c.explained_variance_ratio_)\n",
    "\n",
    "topic_word = pd.DataFrame(lsa_c.components_.round(3),\n",
    "             columns = vectorizer_count.get_feature_names())\n",
    "display_topics(lsa_c, vectorizer_count.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pd.DataFrame(doc_topic_lsa_c).to_pickle('data/lax_lsa_feats.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsa_tf = TruncatedSVD(20)\n",
    "# doc_topic_lsa_t = lsa_tf.fit_transform(doc_term_mat_tf)\n",
    "# print('Explained Variance Ratio per topic:\\n',lsa_tf.explained_variance_ratio_)\n",
    "\n",
    "# topic_word = pd.DataFrame(lsa_tf.components_.round(3),\n",
    "#              columns = vectorizer_tf.get_feature_names())\n",
    "# display_topics(lsa_tf, vectorizer_tf.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "beach, amazing, vacation, sunny, bright, getaway, trendy, charming, breeze, yoga\n",
      "\n",
      "Topic  1\n",
      "spacious, bright, balcony, convenient, fabulous, makes, secure, spaces, plush, closets\n",
      "\n",
      "Topic  2\n",
      "pool, jacuzzi, swimming, heated, luxury, gated, complex, sauna, resort, security\n",
      "\n",
      "Topic  3\n",
      "modern, luxury, design, designed, chic, style, decor, gorgeous, furnishings, designer\n",
      "\n",
      "Topic  4\n",
      "beautiful, beaches, amazing, lovely, relax, conveniently, trees, style, deck, peaceful\n",
      "\n",
      "Topic  5\n",
      "patio, relax, morning, outside, charming, relaxing, work, lovely, driveway, completely\n",
      "\n",
      "Topic  6\n",
      "quiet, safe, peaceful, residential, backyard, hiking, share, transportation, convenient, gated\n",
      "\n",
      "Topic  7\n",
      "views, deck, amazing, gorgeous, panoramic, incredible, stunning, star, balcony, luxury\n",
      "\n",
      "Topic  8\n",
      "cozy, comfy, relax, work, charming, cute, sleep, comes, relaxing, style\n",
      "\n",
      "Topic  9\n",
      "netflix, hulu, prime, hdtv, luxury, roku, comfortably, streaming, gated, sleep\n",
      "\n",
      "Topic  10\n",
      "clean, safe, comfy, privacy, hostel, respect, smoking, casa, security, respectful\n",
      "\n",
      "Topic  11\n",
      "comfortable, work, price, historic, relaxing, ideal, safe, amazing, spaces, quality\n",
      "\n",
      "Topic  12\n",
      "business, couples, solo, adventurers, families, kids, groups, friends, ideal, coziness\n",
      "\n",
      "Topic  13\n",
      "newly, remodeled, renovated, completely, recently, guesthouse, driveway, conveniently, comfortably, units\n",
      "\n",
      "Topic  14\n",
      "fame, theater, amazing, bowl, style, entertainment, historic, theatre, dolby, runyon\n",
      "\n",
      "Topic  15\n",
      "view, balcony, amazing, luxury, deck, panoramic, overlooking, spectacular, complex, security\n",
      "\n",
      "Topic  16\n",
      "family, backyard, friends, single, ideal, relax, kids, beaches, driveway, group\n",
      "\n",
      "Topic  17\n",
      "garden, guesthouse, backyard, trees, lovely, charming, lush, retreat, oasis, fruit\n",
      "\n",
      "Topic  18\n",
      "outdoor, seating, style, deck, fireplace, lounge, backyard, guesthouse, trees, gorgeous\n",
      "\n",
      "Topic  19\n",
      "light, open, natural, bright, airy, work, ceiling, relax, deck, designed\n"
     ]
    }
   ],
   "source": [
    "nmf_c = NMF(20)\n",
    "doc_topic_nmf_c = nmf_c.fit_transform(doc_term_mat_count)\n",
    "# nmf_model.explained_variance_ratio_\n",
    "topic_word = pd.DataFrame(nmf_c.components_.round(3),\n",
    "             columns = vectorizer_count.get_feature_names())\n",
    "display_topics(nmf_c, vectorizer_count.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pd.DataFrame(doc_topic_nmf_c).to_pickle('data/lax_nmf_feats.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14993108e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        3.89339308e-02, 1.18815239e-02, 0.00000000e+00],\n",
       "       [7.46803072e-01, 1.24239736e-01, 0.00000000e+00, ...,\n",
       "        3.70403942e-04, 9.69600911e-03, 0.00000000e+00],\n",
       "       [1.65527478e-01, 6.69432117e-03, 5.79780110e-03, ...,\n",
       "        4.63385777e-03, 1.92926204e-01, 1.06593830e-02],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.68756591e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.67330684e-03, 1.13313112e-02, 0.00000000e+00, ...,\n",
       "        7.78122524e-04, 0.00000000e+00, 6.58660248e-02]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_nmf_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_t = NMF(20)\n",
    "# doc_topic_nmf_t = nmf_t.fit_transform(doc_term_mat_tf)\n",
    "# # nmf_model_tf = doc_topic\n",
    "# # nmf_model.explained_variance_ratio_\n",
    "# topic_word = pd.DataFrame(nmf_t.components_.round(3),\n",
    "#              columns = vectorizer_tf.get_feature_names())\n",
    "# display_topics(nmf_t, vectorizer_tf.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_c = LatentDirichletAllocation(n_components=20,max_iter=50)\n",
    "doc_topic_lda_c = lda_c.fit_transform(doc_term_mat_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "beautifully, heights, professional, decorated, sqft, sure, allows, utilities, theater, class\n",
      "\n",
      "Topic  1\n",
      "newly, remodeled, renovated, recently, clean, completely, security, privacy, gated, units\n",
      "\n",
      "Topic  2\n",
      "views, view, gorgeous, modern, luxury, deck, amazing, luxurious, beautiful, balcony\n",
      "\n",
      "Topic  3\n",
      "outdoor, open, patio, garden, light, modern, style, trees, relax, deck\n",
      "\n",
      "Topic  4\n",
      "pool, jacuzzi, balcony, luxury, swimming, beautiful, complex, view, gated, resort\n",
      "\n",
      "Topic  5\n",
      "netflix, prime, hulu, sleep, hdtv, comfortably, spacious, comfortable, heat, gated\n",
      "\n",
      "Topic  6\n",
      "beach, villa, course, balcony, golf, beaches, view, beautiful, vacation, deck\n",
      "\n",
      "Topic  7\n",
      "duplex, east, wall, historic, light, completely, relax, corner, natural, comfortably\n",
      "\n",
      "Topic  8\n",
      "clean, quiet, safe, cozy, comfortable, convenient, transportation, share, single, family\n",
      "\n",
      "Topic  9\n",
      "quiet, hiking, beautiful, smoking, trails, family, bowl, spacious, rock, patio\n",
      "\n",
      "Topic  10\n",
      "business, couples, solo, families, adventurers, kids, friends, bunk, groups, family\n",
      "\n",
      "Topic  11\n",
      "pets, cleaning, longer, late, clean, work, staying, used, meet, charge\n",
      "\n",
      "Topic  12\n",
      "items, music, tiny, personal, creative, film, photo, unique, cabin, designed\n",
      "\n",
      "Topic  13\n",
      "quiet, patio, beautiful, backyard, garden, guesthouse, cozy, outdoor, family, comfortable\n",
      "\n",
      "Topic  14\n",
      "fame, theater, movie, theatre, runyon, bowl, subway, theaters, amazing, entertainment\n",
      "\n",
      "Topic  15\n",
      "modern, open, spacious, balcony, luxury, lighting, beautiful, designer, bright, entertainment\n",
      "\n",
      "Topic  16\n",
      "spacious, light, beautiful, cozy, quiet, rodeo, natural, strip, bright, comfortable\n",
      "\n",
      "Topic  17\n",
      "comfortable, chair, clean, comes, heater, plates, work, toilet, channels, cups\n",
      "\n",
      "Topic  18\n",
      "beach, cozy, amazing, beautiful, historic, bright, charming, retro, foods, life\n",
      "\n",
      "Topic  19\n",
      "traffic, rent, dinning, farm, clean, movies, cozy, relax, beautiful, sure\n"
     ]
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lda_c.components_.round(3),\n",
    "             columns = vectorizer_count.get_feature_names())\n",
    "display_topics(lda_c, vectorizer_count.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pd.DataFrame(doc_topic_lda_c).to_pickle('data/lax_lda_feats.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "      <td>14522.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.023277</td>\n",
       "      <td>0.040010</td>\n",
       "      <td>0.051795</td>\n",
       "      <td>0.094056</td>\n",
       "      <td>0.046635</td>\n",
       "      <td>0.045474</td>\n",
       "      <td>0.029960</td>\n",
       "      <td>0.039428</td>\n",
       "      <td>0.082174</td>\n",
       "      <td>0.046227</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>0.025906</td>\n",
       "      <td>0.090315</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>0.048094</td>\n",
       "      <td>0.065303</td>\n",
       "      <td>0.050052</td>\n",
       "      <td>0.059387</td>\n",
       "      <td>0.033762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.080585</td>\n",
       "      <td>0.091741</td>\n",
       "      <td>0.115713</td>\n",
       "      <td>0.167998</td>\n",
       "      <td>0.113733</td>\n",
       "      <td>0.118392</td>\n",
       "      <td>0.085672</td>\n",
       "      <td>0.094885</td>\n",
       "      <td>0.158470</td>\n",
       "      <td>0.108927</td>\n",
       "      <td>0.108255</td>\n",
       "      <td>0.109398</td>\n",
       "      <td>0.079835</td>\n",
       "      <td>0.152571</td>\n",
       "      <td>0.108922</td>\n",
       "      <td>0.113935</td>\n",
       "      <td>0.144224</td>\n",
       "      <td>0.104162</td>\n",
       "      <td>0.121789</td>\n",
       "      <td>0.089312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.042475</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.132099</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.024395</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.142296</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.047421</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050349</td>\n",
       "      <td>0.056677</td>\n",
       "      <td>0.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.983036</td>\n",
       "      <td>0.989560</td>\n",
       "      <td>0.991204</td>\n",
       "      <td>0.988953</td>\n",
       "      <td>0.986429</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>0.985606</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.986986</td>\n",
       "      <td>0.984167</td>\n",
       "      <td>0.987333</td>\n",
       "      <td>0.968086</td>\n",
       "      <td>0.979787</td>\n",
       "      <td>0.989674</td>\n",
       "      <td>0.984921</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.970312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  14522.000000  14522.000000  14522.000000  14522.000000  14522.000000   \n",
       "mean       0.023277      0.040010      0.051795      0.094056      0.046635   \n",
       "std        0.080585      0.091741      0.115713      0.167998      0.113733   \n",
       "min        0.000327      0.000336      0.000336      0.000385      0.000327   \n",
       "25%        0.000769      0.000833      0.000862      0.000943      0.000806   \n",
       "50%        0.001163      0.001471      0.001471      0.001786      0.001282   \n",
       "75%        0.003846      0.042475      0.050000      0.132099      0.025000   \n",
       "max        0.983036      0.989560      0.991204      0.988953      0.986429   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  14522.000000  14522.000000  14522.000000  14522.000000  14522.000000   \n",
       "mean       0.045474      0.029960      0.039428      0.082174      0.046227   \n",
       "std        0.118392      0.085672      0.094885      0.158470      0.108927   \n",
       "min        0.000327      0.000336      0.000327      0.000327      0.000327   \n",
       "25%        0.000833      0.000781      0.000806      0.000794      0.000794   \n",
       "50%        0.001351      0.001163      0.001316      0.001471      0.001250   \n",
       "75%        0.025000      0.004167      0.016667      0.100399      0.016667   \n",
       "max        0.987333      0.987333      0.982407      0.985606      0.954762   \n",
       "\n",
       "                 10            11            12            13            14  \\\n",
       "count  14522.000000  14522.000000  14522.000000  14522.000000  14522.000000   \n",
       "mean       0.039040      0.049459      0.025906      0.090315      0.039648   \n",
       "std        0.108255      0.109398      0.079835      0.152571      0.108922   \n",
       "min        0.000327      0.000327      0.000327      0.000327      0.000327   \n",
       "25%        0.000820      0.000820      0.000794      0.000877      0.000781   \n",
       "50%        0.001316      0.001389      0.001190      0.001852      0.001220   \n",
       "75%        0.024395      0.050000      0.004545      0.142296      0.012500   \n",
       "max        0.986986      0.984167      0.987333      0.968086      0.979787   \n",
       "\n",
       "                 15            16            17            18            19  \n",
       "count  14522.000000  14522.000000  14522.000000  14522.000000  14522.000000  \n",
       "mean       0.048094      0.065303      0.050052      0.059387      0.033762  \n",
       "std        0.113935      0.144224      0.104162      0.121789      0.089312  \n",
       "min        0.000336      0.000327      0.000327      0.000345      0.000327  \n",
       "25%        0.000847      0.000794      0.000877      0.000833      0.000769  \n",
       "50%        0.001389      0.001316      0.001563      0.001563      0.001190  \n",
       "75%        0.047421      0.050000      0.050349      0.056677      0.008333  \n",
       "max        0.989674      0.984921      0.981000      0.962000      0.970312  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_topic_lda_c).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_t = LatentDirichletAllocation(n_components=20,max_iter=50)\n",
    "# doc_topic_lda_t = lda_t.fit_transform(doc_term_mat_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_word = pd.DataFrame(lda_t.components_.round(3),\n",
    "#              columns = vectorizer_tf.get_feature_names())\n",
    "# display_topics(lda_t, vectorizer_tf.get_feature_names(), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
